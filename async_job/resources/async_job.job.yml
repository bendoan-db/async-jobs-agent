resources:
  jobs:
    async_job:
      name: async_job

      # Job-level parameters (passed at runtime via jobs.run_now)
      parameters:
        - name: user_request
          default: ""

      # Serverless environment configuration
      environments:
        - environment_key: serverless_env
          spec:
            client: "3"
            dependencies:
              - psycopg2-binary
              - databricks-sdk>=0.61.0
              - databricks-langchain[memory]
              - databricks-agents
              - mlflow-skinny[databricks]
              - pyyaml

      tasks:
        - task_key: agent_workflow_task
          environment_key: serverless_env
          spark_python_task:
            python_file: ../src/agent_workflow_task.py
            parameters:
              - "--lakebase-instance"
              - "${var.lakebase_instance}"
              - "--user-request"
              - "{{job.parameters.user_request}}"

      max_concurrent_runs: 10
